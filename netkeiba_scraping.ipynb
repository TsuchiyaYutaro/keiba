{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsuchiya/.local/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import re, time, json, pickle\n",
    "\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://race.netkeiba.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 開催日URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c004668d25eb45d18bc59bd1585904d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# date_list = []\n",
    "\n",
    "# for y in tqdm(range(2000, 2020+1)):\n",
    "#     for m in range(1, 12+1):\n",
    "#         url = f'{base_url}/top/calendar.html?year={y}&month={m}'\n",
    "        \n",
    "#         html = requests.get(url)\n",
    "#         html.encoding = html.apparent_encoding \n",
    "#         soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "        \n",
    "#         date_list += list(map(lambda x: x.get('href'), soup.find(class_='Calendar_Table').find_all('a')))\n",
    "        \n",
    "#         time.sleep(2 + np.random.rand())\n",
    "        \n",
    "# date_list = list(map(lambda x: x.replace('..', base_url), date_list))\n",
    "# f = open('./data/date_list.pkl', 'wb')\n",
    "# pickle.dump(date_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/date_list.pkl','rb')\n",
    "date_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レースID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https:db.netkeiba.comracesum', 'https:race.netkeiba.com'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(map(lambda x: ''.join(x.split('/')[:-2]), date_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://db.netkeiba.com/race/sum/06/20000105',\n",
       " 'https://db.netkeiba.com/race/sum/08/20000105']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6a0f020f024ed595fbc7158122fa4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\nWebDriverError@chrome://remote/content/shared/webdriver/Errors.jsm:181:5\nNoSuchElementError@chrome://remote/content/shared/webdriver/Errors.jsm:393:5\nelement.find/</<@chrome://remote/content/marionette/element.js:299:16\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dc02d44ab634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mWebDriverWait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisibility_of_element_located\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RaceTopRace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anyenv/envs/pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/selenium/webdriver/support/wait.py\u001b[0m in \u001b[0;36muntil\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0muntil_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \nStacktrace:\nWebDriverError@chrome://remote/content/shared/webdriver/Errors.jsm:181:5\nNoSuchElementError@chrome://remote/content/shared/webdriver/Errors.jsm:393:5\nelement.find/</<@chrome://remote/content/marionette/element.js:299:16\n"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "\n",
    "driver = webdriver.Firefox(options=options)\n",
    "\n",
    "race_list = []\n",
    "\n",
    "for url in tqdm(date_list):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.ID, 'RaceTopRace')))\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    table = soup.find_all(class_='RaceList_Data')\n",
    "    \n",
    "    for t in table:\n",
    "        race_list += list(map(lambda x: x.get('href'), t.find_all('a')))\n",
    "    \n",
    "    time.sleep(2 + np.random.rand())\n",
    "    \n",
    "race_list = list(map(lambda x: x.replace('..', base_url), race_list))\n",
    "race_list = [x for x in race_list if 'movie' not in x]\n",
    "f = open('./data/race_list.pkl', 'wb')\n",
    "pickle.dump(race_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/race_list.pkl','rb')\n",
    "race_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レース詳細"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# race_info = []\n",
    "# horse_url = []\n",
    "\n",
    "# for url in tqdm(race_list):\n",
    "#     html = requests.get(url)\n",
    "#     html.encoding = html.apparent_encoding \n",
    "#     soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "    \n",
    "#     race_id = url.split('race_id=')[-1].split('&rf')[0]\n",
    "#     year = race_id[:4]\n",
    "    \n",
    "#     # レース情報\n",
    "#     race_dict = {}\n",
    "#     race_dict['race_id'] = race_id\n",
    "    \n",
    "#     try: race_dict['place'] = soup.find(class_='RaceKaisaiWrap').find(class_='Active').text\n",
    "#     except: race_dict['place'] = np.nan\n",
    "        \n",
    "#     race_dict['race_name'] = soup.find(class_='RaceName').text.strip()\n",
    "#     race_dict['date'] = year + '年' + soup.find(id='RaceList_DateList').find(class_='Active').text\n",
    "#     race_dict['race_num'] = soup.find(class_='RaceNumWrap').find(class_='Active').text.strip()\n",
    "#     race_dict['start'] = list(map(lambda x: x.strip(), soup.find(class_='RaceData01').text.split('/')))[0]\n",
    "#     race_dict['condition'] = list(map(lambda x: x.strip(), soup.find(class_='RaceData01').text.split('/')))[1]\n",
    "#     race_dict['weather'] = list(map(lambda x: x.strip(), soup.find(class_='RaceData01').text.split('/')))[2]\n",
    "#     race_dict['baba'] = list(map(lambda x: x.strip(), soup.find(class_='RaceData01').text.split('/')))[3]\n",
    "    \n",
    "#     table = soup.find_all(class_='Payout_Detail_Table')\n",
    "#     for t in table:\n",
    "#         for t2 in t.find_all('tr'):\n",
    "#             label = t2.find('th').text\n",
    "#             if label in ['複勝', 'ワイド']:\n",
    "#                 value = t2.find_all('td')[1].text.split('円')[:-1]\n",
    "#             else:\n",
    "#                 value = t2.find_all('td')[1].text.strip()\n",
    "#             race_dict[label] = value\n",
    "    \n",
    "#     race_info.append(race_dict)\n",
    "    \n",
    "#     # レース結果\n",
    "#     try:\n",
    "#         soup_table = soup.find(id='All_Result_Table').find('tbody').find_all('tr')\n",
    "#     except:\n",
    "#         time.sleep(2 + np.random.rand())\n",
    "#         continue\n",
    "        \n",
    "#     r_list = []\n",
    "#     for t1 in soup_table: \n",
    "#         t2 = t1.find_all('td')\n",
    "\n",
    "#         r_dict = {}\n",
    "#         for i, col in enumerate(['result', 'waku', 'umaban', 'horse_name', 'age', 'handicap', 'jockey', 'time',\n",
    "#                                 'diff', 'popular', 'odds', '3F', 'corner', 'trainer', 'weight']):\n",
    "#             r_dict[col] = t2[i].text.strip()\n",
    "#             if col == 'horse_name': \n",
    "#                 hurl = t2[i].find('a').get('href')\n",
    "#                 horse_url.append(hurl)\n",
    "                \n",
    "#         r_dict['horse_id'] = hurl.split('/')[-1]\n",
    "#         r_list.append(r_dict)\n",
    "        \n",
    "#     pd.DataFrame(r_list).to_csv(f'./data/race/{race_id}.csv', index=None)\n",
    "    \n",
    "#     horse_url = list(set(horse_url))\n",
    "#     time.sleep(2 + np.random.rand())\n",
    "    \n",
    "# pd.DataFrame(race_info).to_csv('./data/race_info.csv', index=None)\n",
    "\n",
    "# f = open('./data/horse_url.pkl', 'wb')\n",
    "# pickle.dump(horse_url, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/horse_url.pkl','rb')\n",
    "race_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ウマ情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fac9f36cb8499eaca7ac95f739c3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "horse_info = {}\n",
    "\n",
    "for url in tqdm(race_list):\n",
    "    horse_id = url.split('/')[-1]\n",
    "    horse_info['horse_id'] = horse_id\n",
    "    \n",
    "    html = requests.get(url)\n",
    "    html.encoding = html.apparent_encoding \n",
    "    soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "    \n",
    "    horse_info['name'] = soup.find(class_='horse_title').find('h1').text.strip()\n",
    "    horse_info['detail'] = soup.find(class_='horse_title').find(class_='txt_01').text.strip().split('\\u3000')\n",
    "    horse_info['trainer'] = soup.find(summary='のプロフィール').find_all('td')[1].text\n",
    "    horse_info['owner'] = soup.find(summary='のプロフィール').find_all('td')[2].text\n",
    "    horse_info['sire'] = soup.find(class_='blood_table').find_all(class_='b_ml')[0].text.strip()\n",
    "    horse_info['mare_sire'] = soup.find(class_='blood_table').find_all(class_='b_ml')[2].text.strip()\n",
    "    \n",
    "    race_result_list = []\n",
    "\n",
    "    try: table = soup.find(class_='db_h_race_results').find('tbody').find_all('tr')\n",
    "    except: pass\n",
    "\n",
    "    for t in table:\n",
    "        race_result = {}\n",
    "        race_result['date'] = t.find_all('td')[0].text\n",
    "        race_result['place'] = t.find_all('td')[1].text\n",
    "        race_result['weather'] = t.find_all('td')[2].text\n",
    "        race_result['name'] = t.find_all('td')[4].text\n",
    "        race_result['waku'] = t.find_all('td')[7].text\n",
    "        race_result['num'] = t.find_all('td')[8].text\n",
    "        race_result['odds'] = t.find_all('td')[9].text\n",
    "        race_result['pop'] = t.find_all('td')[10].text\n",
    "        race_result['result'] = t.find_all('td')[11].text\n",
    "        race_result['jockey'] = t.find_all('td')[12].text.strip()\n",
    "        race_result['kinryou'] = t.find_all('td')[13].text\n",
    "        race_result['dist'] = t.find_all('td')[14].text\n",
    "        race_result['baba'] = t.find_all('td')[15].text\n",
    "        race_result['time'] = t.find_all('td')[17].text\n",
    "        race_result['sa'] = t.find_all('td')[18].text\n",
    "        race_result['nobori'] = t.find_all('td')[22].text\n",
    "        race_result['weight'] = t.find_all('td')[23].text\n",
    "        race_result_list.append(race_result)\n",
    "\n",
    "    pd.DataFrame(race_result_list).to_csv(f'./data/horse/{horse_id}.csv')\n",
    "    \n",
    "    time.sleep(2 + np.random.rand())\n",
    "    \n",
    "pd.DataFrame(horse_info).to_csv(f'./data/horse_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
